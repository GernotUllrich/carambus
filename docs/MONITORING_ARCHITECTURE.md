# Scraping Monitoring - Architektur ğŸ—ï¸

Visuelle Ãœbersicht des Monitoring-Systems.

---

## ğŸ“Š System-Ãœbersicht

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CARAMBUS SCRAPING MONITOR                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Scraping Operations        â”‚
              â”‚  (Tournament, Location, etc) â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ include ScrapeMonitor
                             â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   ScrapingMonitor            â”‚
              â”‚  (Concern)                   â”‚
              â”‚                              â”‚
              â”‚  - track_scraping()          â”‚
              â”‚  - record_created()          â”‚
              â”‚  - record_updated()          â”‚
              â”‚  - record_deleted()          â”‚
              â”‚  - record_error()            â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ Persists to DB
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   scraping_logs Table        â”‚
              â”‚  (PostgreSQL)                â”‚
              â”‚                              â”‚
              â”‚  - operation                 â”‚
              â”‚  - duration                  â”‚
              â”‚  - created_count             â”‚
              â”‚  - updated_count             â”‚
              â”‚  - error_count               â”‚
              â”‚  - errors_json               â”‚
              â”‚  - executed_at               â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ Queried by
                         â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                            â”‚
           â–¼                            â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Web Dashboard â”‚         â”‚   CLI Tools         â”‚
  â”‚                â”‚         â”‚  (Rake Tasks)       â”‚
  â”‚  /scraping_    â”‚         â”‚                     â”‚
  â”‚   monitor      â”‚         â”‚  - scrape:stats     â”‚
  â”‚                â”‚         â”‚  - scrape:health    â”‚
  â”‚  - Overview    â”‚         â”‚  - scrape:errors    â”‚
  â”‚  - Per-Op View â”‚         â”‚  - scrape:export    â”‚
  â”‚  - Auto-Refreshâ”‚         â”‚                     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Monitoring-Ablauf

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SCRAPING WITH MONITORING                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. START SCRAPING
   â”œâ”€ rake scrape:daily_update_monitored
   â”‚
   â””â”€â–¶ ScrapingMonitor.new("daily_update")
       â”‚
       â”œâ”€ start_time = Time.current
       â”œâ”€ stats = { created: 0, updated: 0, ... }
       â””â”€ errors = []

2. EXECUTE SCRAPING
   â”‚
   â”œâ”€â–¶ Tournament.scrape_single_tournament_public
   â”‚   â”‚
   â”‚   â”œâ”€ Success? â†’ monitor.record_updated(tournament)
   â”‚   â”‚             â†’ stats[:updated] += 1
   â”‚   â”‚
   â”‚   â””â”€ Error?   â†’ monitor.record_error(tournament, exception)
   â”‚                 â†’ stats[:errors] += 1
   â”‚                 â†’ errors << { record, error, backtrace }
   â”‚
   â”œâ”€â–¶ Location.scrape_locations
   â”‚   â””â”€ monitor.track_method("scrape_locations")
   â”‚
   â””â”€â–¶ Club.scrape_clubs
       â””â”€ monitor.track_method("scrape_clubs")

3. SAVE RESULTS
   â”‚
   â””â”€â–¶ ScrapingLog.create!(
         operation: "daily_update",
         duration: 125.3,
         created_count: 123,
         updated_count: 456,
         error_count: 5,
         errors_json: [...],
         executed_at: start_time
       )

4. LOG OUTPUT
   â”‚
   â”œâ”€â–¶ Rails.logger.info "ğŸ“Š ScrapeMonitor Summary"
   â”œâ”€â–¶ Rails.logger.info "   Duration: 125.3s"
   â”œâ”€â–¶ Rails.logger.info "   Created: 123"
   â”œâ”€â–¶ Rails.logger.info "   Updated: 456"
   â””â”€â–¶ Rails.logger.info "   Errors: 5"
```

---

## ğŸ“ˆ Datenfluss: Statistiken

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   STATISTICS DATA FLOW                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

scraping_logs Table
  â”œâ”€ 1000+ Log-EintrÃ¤ge
  â”‚
  â”œâ”€â–¶ ScrapingLog.stats_for("daily_update", since: 7.days.ago)
  â”‚   â”‚
  â”‚   â””â”€â–¶ Aggregation:
  â”‚       â”œâ”€ total_runs = logs.count
  â”‚       â”œâ”€ avg_duration = logs.average(:duration)
  â”‚       â”œâ”€ total_created = logs.sum(:created_count)
  â”‚       â”œâ”€ total_updated = logs.sum(:updated_count)
  â”‚       â”œâ”€ total_errors = logs.sum(:error_count)
  â”‚       â””â”€ success_rate = (ops - errors) / ops * 100
  â”‚
  â”œâ”€â–¶ ScrapingLog.all_operations_stats
  â”‚   â”‚
  â”‚   â””â”€â–¶ Group by operation:
  â”‚       â”œâ”€ "daily_update" â†’ stats
  â”‚       â”œâ”€ "region_scraping" â†’ stats
  â”‚       â””â”€ "tournament_scraping" â†’ stats
  â”‚
  â””â”€â–¶ ScrapingLog.check_anomalies
      â”‚
      â””â”€â–¶ PrÃ¼fungen:
          â”œâ”€ success_rate < 90% ? â†’ ALARM
          â”œâ”€ avg_duration > 300s ? â†’ ALARM
          â””â”€ no recent runs ? â†’ ALARM
```

---

## ğŸš¨ Anomalie-Erkennung

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ANOMALY DETECTION                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

rake scrape:check_health
  â”‚
  â””â”€â–¶ ScrapingLog.check_anomalies(threshold: 0.1)
      â”‚
      â”œâ”€â–¶ For each operation:
      â”‚   â”‚
      â”‚   â”œâ”€ Check 1: High Error Rate?
      â”‚   â”‚   â”‚
      â”‚   â”‚   â”œâ”€ success_rate < 90%
      â”‚   â”‚   â””â”€â–¶ YES â†’ anomaly << {
      â”‚   â”‚           operation: "daily_update",
      â”‚   â”‚           issue: "High error rate: 15%"
      â”‚   â”‚         }
      â”‚   â”‚
      â”‚   â””â”€ Check 2: Slow Performance?
      â”‚       â”‚
      â”‚       â”œâ”€ avg_duration > 300s
      â”‚       â””â”€â–¶ YES â†’ anomaly << {
      â”‚               operation: "region_scraping",
      â”‚               issue: "Slow: 425s"
      â”‚             }
      â”‚
      â””â”€â–¶ Return anomalies[]
          â”‚
          â”œâ”€ Empty? â†’ Exit 0 (OK)
          â””â”€ Found? â†’ Exit 1 (ALARM)
                      â†’ Log to Rails.logger
                      â†’ Send to monitoring system
```

---

## ğŸŒ Web-Dashboard Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      WEB DASHBOARD                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

HTTP GET /scraping_monitor
  â”‚
  â””â”€â–¶ ScrapingMonitorController#index
      â”‚
      â”œâ”€â–¶ @time_range = params[:days] || 7
      â”‚
      â”œâ”€â–¶ @operations_stats = ScrapingLog.all_operations_stats(since: 7.days.ago)
      â”‚   â””â”€â–¶ [
      â”‚         { operation: "daily_update", total_runs: 7, ... },
      â”‚         { operation: "region_scraping", total_runs: 42, ... }
      â”‚       ]
      â”‚
      â”œâ”€â–¶ @recent_logs = ScrapingLog.recent(50)
      â”‚
      â”œâ”€â–¶ @anomalies = ScrapingLog.check_anomalies
      â”‚
      â””â”€â–¶ render :index
          â”‚
          â””â”€â–¶ HTML:
              â”œâ”€ Anomaly Alert (rot/grÃ¼n)
              â”œâ”€ Stats Cards (Created/Updated/Deleted/Errors)
              â”œâ”€ Operations Table (sortierbar)
              â””â”€ Recent Logs Table (letzte 50)

              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ ğŸ” Scraping Monitor Dashboard   â”‚
              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
              â”‚ âœ… All Systems Operational      â”‚
              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
              â”‚ Created: 1234 | Updated: 5678   â”‚
              â”‚ Deleted: 12   | Errors: 5       â”‚
              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
              â”‚ Operations (Last 7 Days)        â”‚
              â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
              â”‚ â”‚ daily_update  â”‚ 7 runs    â”‚   â”‚
              â”‚ â”‚ Ã˜ 125s        â”‚ 98.9% âœ“   â”‚   â”‚
              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

              Auto-Refresh: 30s

HTTP GET /scraping_monitor/daily_update
  â”‚
  â””â”€â–¶ ScrapingMonitorController#operation
      â”‚
      â”œâ”€â–¶ @operation = "daily_update"
      â”œâ”€â–¶ @stats = ScrapingLog.stats_for(@operation)
      â”œâ”€â–¶ @logs = ScrapingLog.by_operation(@operation).limit(100)
      â”‚
      â””â”€â–¶ render :operation
          â”‚
          â””â”€â–¶ HTML:
              â”œâ”€ Stats Cards (Runs, Duration, Created, ...)
              â”œâ”€ Execution History Table
              â””â”€ Error Details (aufklappbar)
```

---

## ğŸ”§ Code-Integration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HOW TO INTEGRATE INTO YOUR CODE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OPTION 1: Include Concern
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Tournament < ApplicationRecord
  include ScrapeMonitor  â† Add this
  
  def my_scraping_method
    track_scraping("tournament_scraping") do |monitor|
      # Your code
      tournament = scrape_data()
      
      if tournament.new_record?
        monitor.record_created(tournament)
      else
        monitor.record_updated(tournament)
      end
      
      monitor.track_method("scrape_data")
    end
  end
end


OPTION 2: Manual Monitoring
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

monitor = ScrapingMonitor.new("my_operation", "Context")

monitor.run do |m|
  begin
    records.each do |record|
      result = process(record)
      m.record_updated(record) if result
    end
  rescue => e
    m.record_error(record, e)
  end
  
  m.track_method("process")
end


CALLBACKS AVAILABLE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

monitor.record_created(record)     â†’ stats[:created] += 1
monitor.record_updated(record)     â†’ stats[:updated] += 1
monitor.record_deleted(record)     â†’ stats[:deleted] += 1
monitor.record_unchanged(record)   â†’ stats[:unchanged] += 1
monitor.record_error(record, err)  â†’ stats[:errors] += 1
monitor.track_method("method_name") â†’ method_calls << "method_name"
```

---

## ğŸ“¦ Dateien-Ãœbersicht

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FILES CREATED                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Backend:
â”œâ”€â”€ app/models/concerns/scraping_monitor.rb         [Concern]
â”œâ”€â”€ app/models/scraping_log.rb                      [Model]
â”œâ”€â”€ db/migrate/20260215194955_create_scraping_logs.rb
â””â”€â”€ db/migrate/20260215195121_add_unchanged_count_to_scraping_logs.rb

Frontend:
â”œâ”€â”€ app/controllers/scraping_monitor_controller.rb  [Controller]
â”œâ”€â”€ app/views/scraping_monitor/index.html.erb       [Dashboard]
â”œâ”€â”€ app/views/scraping_monitor/operation.html.erb   [Detail View]
â””â”€â”€ config/routes.rb                                [+2 Routes]

CLI:
â””â”€â”€ lib/tasks/scrape_monitored.rake                 [7 Tasks]

Documentation:
â”œâ”€â”€ docs/SCRAPING_MONITORING.md                     [VollstÃ¤ndig]
â”œâ”€â”€ docs/SCRAPING_MONITORING_QUICKSTART.md          [5 Min]
â”œâ”€â”€ docs/MONITORING_ARCHITECTURE.md                 [This file]
â””â”€â”€ MONITORING_SYSTEM.md                            [Ãœbersicht]

Updated:
â””â”€â”€ test/README.md                                  [+Monitoring Link]
```

---

## ğŸ¯ Deployment-Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEPLOYMENT WORKFLOW                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LOCAL DEVELOPMENT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. bin/rails db:migrate
2. rake scrape:daily_update_monitored
3. open http://localhost:3000/scraping_monitor


SCENARIO DEPLOYMENT (per carambus_bcw etc):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. cd /Volumes/EXT2TB/gullrich/DEV/carambus/carambus_master
2. [Make changes & commit]
3. git push

4. cd /Volumes/EXT2TB/gullrich/DEV/carambus/carambus_bcw
5. git pull
6. rake "scenario:deploy[carambus_bcw]"
   â””â”€â–¶ Runs migrations automatically
   â””â”€â–¶ Restarts Puma


PRODUCTION CRON JOBS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Daily Scraping (3:00 AM)
0 3 * * * cd /var/www/carambus_bcw && rake scrape:daily_update_monitored

# Health Check (6:00 AM)
0 6 * * * cd /var/www/carambus_bcw && rake scrape:check_health || \
          mail -s "Scraping Alert: carambus_bcw" admin@example.com

# Weekly Cleanup (Sunday 4:00 AM)
0 4 * * 0 cd /var/www/carambus_bcw && rake scrape:cleanup_logs[90]
```

---

## ğŸ’¾ Datenbank-Schema

```sql
CREATE TABLE scraping_logs (
  id               BIGSERIAL PRIMARY KEY,
  operation        VARCHAR NOT NULL,
  context          VARCHAR,
  duration         FLOAT,
  created_count    INTEGER DEFAULT 0,
  updated_count    INTEGER DEFAULT 0,
  deleted_count    INTEGER DEFAULT 0,
  unchanged_count  INTEGER DEFAULT 0,
  error_count      INTEGER DEFAULT 0,
  errors_json      TEXT,
  executed_at      TIMESTAMP NOT NULL,
  created_at       TIMESTAMP NOT NULL,
  updated_at       TIMESTAMP NOT NULL
);

CREATE INDEX index_scraping_logs_on_operation ON scraping_logs(operation);
CREATE INDEX index_scraping_logs_on_executed_at ON scraping_logs(executed_at);
CREATE INDEX index_scraping_logs_on_operation_and_executed_at 
  ON scraping_logs(operation, executed_at);
```

---

## ğŸ”® ErweiterungsmÃ¶glichkeiten

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FUTURE EXTENSIONS                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 1: Alerting
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ScrapingLog â”‚
â”‚  .anomalies â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â–¶ Slack Webhook
       â”‚   POST https://hooks.slack.com/...
       â”‚   { text: "âš ï¸ High error rate in daily_update" }
       â”‚
       â”œâ”€â–¶ Discord Webhook
       â”‚   POST https://discord.com/api/webhooks/...
       â”‚
       â””â”€â–¶ Email (ActionMailer)
           ScrapingAlertMailer.anomaly_detected(anomalies).deliver_now


Phase 2: Metrics Export
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GET /metrics (Prometheus format)
  â”‚
  â””â”€â–¶ # TYPE scraping_duration_seconds gauge
      scraping_duration_seconds{operation="daily_update"} 125.3
      
      # TYPE scraping_errors_total counter
      scraping_errors_total{operation="daily_update"} 5


Phase 3: Advanced Visualizations
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chart.js    â”‚
â”‚  Integration â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â–¶ Performance Chart (Zeit-Serie)
       â”œâ”€â–¶ Error Rate Chart (Trend)
       â””â”€â–¶ Operations Comparison (Bar Chart)


Phase 4: Automatic Retry
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
monitor.run do |m|
  begin
    scrape_data()
  rescue Timeout::Error, Net::HTTPServerError => e
    m.record_error(self, e)
    
    # Automatic Retry with Exponential Backoff
    retry_with_backoff(max_retries: 3) do
      scrape_data()
    end
  end
end
```

---

**ğŸš€ Architecture Complete!**

*"Build systems, not tests."* ğŸ—ï¸âœ¨
